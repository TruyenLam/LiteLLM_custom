# LiteLLM Custom - AI Model Gateway

ğŸš€ **LiteLLM proxy server vá»›i AIMLAPI integration** - Má»™t gateway thá»‘ng nháº¥t cho nhiá»u AI models qua má»™t API endpoint duy nháº¥t.

## âœ¨ Features

- ğŸ”„ **Multi-Provider Support**: OpenAI, Anthropic, Google, Meta models via AIMLAPI
- ğŸŒ **Unified API**: OpenAI-compatible endpoints cho táº¥t cáº£ models
- ğŸ“Š **Database Persistence**: PostgreSQL cho model management vÃ  logging
- ğŸ” **Secure**: API key authentication vÃ  environment variable protection
- ğŸ“ˆ **Monitoring**: Health checks vÃ  metrics vá»›i Prometheus
- ğŸš€ **Railway Ready**: Optimized cho Railway.app deployment

## ğŸ¯ Supported Models

### Via AIMLAPI Integration:
- **ChatGPT-4o Latest** - Most capable OpenAI model
- **ChatGPT-4o** - Advanced reasoning vÃ  vision
- **ChatGPT-4o Mini** - Cost-effective option
- **GPT-4 Turbo** - High performance
- **Claude 3.5 Sonnet** - Excellent reasoning (Anthropic)
- **Gemini 1.5 Pro** - Google's flagship model
- **Llama 3.1 70B/8B** - Meta's advanced models

## ğŸš€ Quick Deploy to Railway

[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app)

### 1. Fork this repo
```bash
git clone https://github.com/yourusername/LiteLLM_custom.git
cd LiteLLM_custom
```

### 2. Deploy to Railway
1. Connect GitHub repo to Railway
2. Set environment variables:
   ```
   AIMLAPI_KEY=your_aimlapi_key
   LITELLM_MASTER_KEY=your_chosen_master_key
   LITELLM_SALT_KEY=your_salt_key
   DATABASE_URL=postgresql://user:pass@host:port/db
   STORE_MODEL_IN_DB=True
   ```
3. Railway sáº½ tá»± Ä‘á»™ng deploy tá»« `Dockerfile.railway`

## ğŸ”§ Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `AIMLAPI_KEY` | API key tá»« AIMLAPI | âœ… |
| `LITELLM_MASTER_KEY` | Master key cho authentication | âœ… |
| `LITELLM_SALT_KEY` | Salt key cho encryption/decryption | âœ… |
| `DATABASE_URL` | PostgreSQL connection string | âœ… |
| `STORE_MODEL_IN_DB` | Store models in database | âœ… |

## ğŸ“¡ API Usage

### Chat Completions
```bash
curl -X POST https://your-railway-domain.railway.app/v1/chat/completions \
  -H "Authorization: Bearer YOUR_MASTER_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "chatgpt-4o-latest",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

### List Available Models
```bash
curl -H "Authorization: Bearer YOUR_MASTER_KEY" \
     https://your-railway-domain.railway.app/v1/models
```

### Health Check
```bash
curl https://your-railway-domain.railway.app/health/liveliness
```

## ğŸ—ï¸ Local Development

### Prerequisites
- Docker & Docker Compose
- Python 3.8+ (optional, for scripts)

### Run Local Server
```bash
# Clone repo
git clone https://github.com/yourusername/LiteLLM_custom.git
cd LiteLLM_custom

# Copy environment file
cp .env.example .env
# Edit .env with your keys

# Start with Docker Compose
docker compose up -d

# Server available at http://localhost:4000
```

### Test Local Server
```bash
curl -X POST http://localhost:4000/v1/chat/completions \
  -H "Authorization: Bearer YOUR_MASTER_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "chatgpt-4o-latest", 
    "messages": [{"role": "user", "content": "Test message"}]
  }'
```

## ğŸ“Š Monitoring

- **Health Check**: `/health/liveliness`
- **Metrics**: `/metrics` (Prometheus format)
- **Model Info**: `/model/info`
- **Database**: Models Ä‘Æ°á»£c lÆ°u trong PostgreSQL cho persistence

## ğŸ”§ Configuration

Xem `railway_config.yaml` Ä‘á»ƒ customize:
- Model parameters
- Timeout settings  
- Retry logic
- Database settings

## ğŸ” Security

- âœ… HTTPS/TLS encryption
- âœ… API key authentication required
- âœ… Environment variables cho sensitive data
- âœ… Request logging cho auditing

## ğŸ’° Cost Optimization

- ğŸŒ **Multi-provider**: Chá»n model tá»‘t nháº¥t cho tá»«ng task
- ğŸ“Š **Usage tracking**: Monitor costs qua database logs
- âš¡ **Efficient routing**: Load balancing vÃ  failover
- ğŸ¯ **Model variety**: Tá»« cost-effective Ä‘áº¿n high-performance

## ğŸ› ï¸ Tech Stack

- **LiteLLM**: Proxy server vÃ  model gateway
- **AIMLAPI**: AI model provider vá»›i 200+ models
- **PostgreSQL**: Database persistence
- **Railway**: Cloud deployment platform
- **Docker**: Containerization
- **Prometheus**: Metrics vÃ  monitoring

## ğŸ“ˆ Performance

- âš¡ **Low latency**: Direct API calls vá»›i minimal overhead
- ğŸ”„ **Auto-retry**: Built-in retry logic cho reliability
- ğŸ“Š **Load balancing**: Distribute requests across models
- ğŸš€ **Scalable**: Railway auto-scaling

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Make changes
4. Test thoroughly
5. Submit pull request

## ğŸ“ License

MIT License - xem [LICENSE](LICENSE) file

## ğŸ†˜ Support

- ğŸ“– [LiteLLM Docs](https://docs.litellm.ai/)
- ğŸš‚ [Railway Docs](https://docs.railway.app/)
- ğŸ¤– [AIMLAPI Docs](https://aimlapi.com/docs)
- ğŸ’¬ [Issues](https://github.com/yourusername/LiteLLM_custom/issues)

---

**Made with â¤ï¸ for the AI community**
