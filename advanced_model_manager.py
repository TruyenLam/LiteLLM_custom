#!/usr/bin/env python3
"""
ğŸš€ SHAREAPIAI.COM - ADVANCED MODEL MANAGER
Quáº£n lÃ½ models tá»« AIMLAPI vÃ  LiteLLM Container
"""

import requests
import json
import os
import time
import random
from datetime import datetime
from typing import List, Dict, Any
import yaml

class ShareAPIAIModelManager:
    def __init__(self):
        self.aimlapi_key = "b0197edcd9104cd1ab78aaf148ce609a"
        self.litellm_base_url = "https://call.shareapiai.com"
        self.litellm_api_key = "sk-hWv1u2fX3yG4zJ5kT6p7qR8sT9uV0wX1"
        self.models_file = "shareapiai_models_info.json"
        
        # Headers for AIMLAPI
        self.aimlapi_headers = {
            "Authorization": f"Bearer {self.aimlapi_key}",
            "Content-Type": "application/json"
        }
        
        # Headers for LiteLLM
        self.litellm_headers = {
            "Authorization": f"Bearer {self.litellm_api_key}",
            "Content-Type": "application/json"
        }
    
    def fetch_latest_models_from_aimlapi(self) -> List[Dict]:
        """
        ğŸ”„ CHá»¨C NÄ‚NG 1: Láº¥y thÃ´ng tin models má»›i nháº¥t tá»« AIMLAPI
        """
        print("ğŸ”„ Äang láº¥y danh sÃ¡ch models má»›i nháº¥t tá»« AIMLAPI...")
        
        try:
            response = requests.get(
                "https://api.aimlapi.com/models",
                headers=self.aimlapi_headers,
                timeout=30
            )
            response.raise_for_status()
            
            models_data = response.json()
            models = models_data.get('data', [])
            
            # ThÃªm thÃ´ng tin shareapiai.com vÃ o má»—i model
            enhanced_models = []
            for model in models:
                enhanced_model = {
                    "id": model.get('id', ''),
                    "object": model.get('object', 'model'),
                    "created": model.get('created', int(time.time())),
                    "owned_by": model.get('owned_by', 'aimlapi'),
                    "description": f"{model.get('id', '')} - Provided by shareapiai.com ",
                    "provider": "shareapiai.com",
                    "shareapiai_enhanced": True,
                    "shareapiai_added_date": datetime.now().isoformat(),
                    "api_base": "https://api.aimlapi.com/v1",
                    "max_tokens": self._get_model_max_tokens(model.get('id', '')),
                    "supports_function_calling": self._supports_function_calling(model.get('id', '')),
                    "supports_vision": self._supports_vision(model.get('id', '')),
                    "model_type": self._get_model_type(model.get('id', ''))
                }
                enhanced_models.append(enhanced_model)
            
            # LÆ°u vÃ o file
            models_info = {
                "updated_at": datetime.now().isoformat(),
                "source": "https://api.aimlapi.com/models",
                "enhanced_by": "shareapiai.com",
                "total_models": len(enhanced_models),
                "models": enhanced_models
            }
            
            with open(self.models_file, 'w', encoding='utf-8') as f:
                json.dump(models_info, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… ÄÃ£ lÆ°u {len(enhanced_models)} models vÃ o {self.models_file}")
            print(f"ğŸ“ File Ä‘Æ°á»£c cáº­p nháº­t lÃºc: {models_info['updated_at']}")
            
            return enhanced_models
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ Lá»—i khi láº¥y models tá»« AIMLAPI: {e}")
            return []
        except Exception as e:
            print(f"âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {e}")
            return []
    
    def _get_model_max_tokens(self, model_id: str) -> int:
        """Æ¯á»›c tÃ­nh max tokens dá»±a trÃªn model name"""
        model_id = model_id.lower()
        if 'gpt-4' in model_id:
            if 'turbo' in model_id or '1106' in model_id or '0125' in model_id:
                return 128000
            return 8192
        elif 'gpt-3.5' in model_id:
            if 'turbo' in model_id and ('1106' in model_id or '0125' in model_id):
                return 16385
            return 4096
        elif 'claude' in model_id:
            if 'claude-3' in model_id:
                return 200000
            return 100000
        elif 'gemini' in model_id:
            if '1.5' in model_id:
                return 2097152
            return 32768
        elif 'llama' in model_id:
            if '70b' in model_id or '405b' in model_id:
                return 128000
            return 8192
        else:
            return 4096
    
    def _supports_function_calling(self, model_id: str) -> bool:
        """Kiá»ƒm tra model cÃ³ há»— trá»£ function calling khÃ´ng"""
        model_id = model_id.lower()
        function_calling_models = [
            'gpt-4', 'gpt-3.5-turbo', 'claude-3', 'gemini-1.5', 
            'llama-3.1', 'mistral', 'command-r'
        ]
        return any(model_name in model_id for model_name in function_calling_models)
    
    def _supports_vision(self, model_id: str) -> bool:
        """Kiá»ƒm tra model cÃ³ há»— trá»£ vision khÃ´ng"""
        model_id = model_id.lower()
        vision_models = [
            'gpt-4o', 'gpt-4-vision', 'claude-3', 'gemini-1.5-pro',
            'llava', 'qwen-vl'
        ]
        return any(model_name in model_id for model_name in vision_models)
    
    def _get_model_type(self, model_id: str) -> str:
        """XÃ¡c Ä‘á»‹nh loáº¡i model"""
        model_id = model_id.lower()
        if any(x in model_id for x in ['gpt', 'openai']):
            return 'openai'
        elif 'claude' in model_id:
            return 'anthropic'
        elif 'gemini' in model_id:
            return 'google'
        elif 'llama' in model_id:
            return 'meta'
        elif 'mistral' in model_id:
            return 'mistral'
        elif 'command' in model_id:
            return 'cohere'
        else:
            return 'other'
    
    def get_current_litellm_models(self) -> List[str]:
        """Láº¥y danh sÃ¡ch models hiá»‡n táº¡i trong LiteLLM"""
        try:
            response = requests.get(
                f"{self.litellm_base_url}/v1/models",
                headers=self.litellm_headers,
                timeout=30
            )
            response.raise_for_status()
            
            models_data = response.json()
            return [model['id'] for model in models_data.get('data', [])]
            
        except Exception as e:
            print(f"âŒ Lá»—i khi láº¥y models tá»« LiteLLM: {e}")
            return []
    
    def update_models_to_container(self) -> None:
        """
        ğŸ”„ CHá»¨C NÄ‚NG 2: Cáº­p nháº­t models tá»« file vÃ o container
        """
        print("ğŸ”„ Äang cáº­p nháº­t models vÃ o LiteLLM container...")
        
        # Äá»c file models
        if not os.path.exists(self.models_file):
            print(f"âŒ File {self.models_file} khÃ´ng tá»“n táº¡i. Cháº¡y chá»©c nÄƒng 1 trÆ°á»›c!")
            return
        
        try:
            with open(self.models_file, 'r', encoding='utf-8') as f:
                models_info = json.load(f)
            
            available_models = models_info.get('models', [])
            current_models = self.get_current_litellm_models()
            
            print(f"ğŸ“Š CÃ³ {len(available_models)} models trong file")
            print(f"ğŸ“Š CÃ³ {len(current_models)} models trong container")
            
            # Danh sÃ¡ch models phá»• biáº¿n Ä‘á»ƒ Æ°u tiÃªn thÃªm
            priority_models = [
                'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-3.5-turbo',
                'claude-3-5-sonnet', 'claude-3-opus', 'claude-3-haiku',
                'gemini-1.5-pro', 'gemini-1.5-flash',
                'llama-3.1-405b', 'llama-3.1-70b', 'llama-3.1-8b',
                'mistral-large', 'command-r-plus'
            ]
            
            added_count = 0
            updated_count = 0
            
            for model in available_models:
                model_id = model.get('id', '')
                
                # Æ¯u tiÃªn models phá»• biáº¿n
                is_priority = any(priority in model_id.lower() for priority in priority_models)
                
                if not is_priority and added_count >= 200:  # Giá»›i háº¡n 20 models Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i
                    continue
                
                if model_id in current_models:
                    print(f"ğŸ”„ Model {model_id} Ä‘Ã£ tá»“n táº¡i, bá» qua...")
                    updated_count += 1
                    continue
                
                # ThÃªm model má»›i
                litellm_config = {
                    "model_name": model_id,
                    "litellm_params": {
                        "model": f"openai/{model_id}",
                        "api_base": model.get('api_base', 'https://api.aimlapi.com/v1'),
                        "api_key": "os.environ/AIMLAPI_KEY"
                    },
                    "model_info": {
                        "description": model.get('description', f"{model_id} via shareapiai.com"),
                        "provider": "shareapi",
                        "max_tokens": model.get('max_tokens', 4096),
                        "supports_function_calling": model.get('supports_function_calling', False),
                        "supports_vision": model.get('supports_vision', False),
                        "model_type": model.get('model_type', 'other'),
                        "shareapiai_enhanced": True
                    }
                }
                
                try:
                    response = requests.post(
                        f"{self.litellm_base_url}/model/new",
                        headers=self.litellm_headers,
                        json=litellm_config,
                        timeout=30
                    )
                    
                    if response.status_code == 200:
                        print(f"âœ… ÄÃ£ thÃªm model: {model_id}")
                        added_count += 1
                    else:
                        print(f"âš ï¸ KhÃ´ng thá»ƒ thÃªm {model_id}: {response.status_code}")
                        
                except Exception as e:
                    print(f"âŒ Lá»—i khi thÃªm {model_id}: {e}")
                
                # Delay Ä‘á»ƒ trÃ¡nh rate limit
                time.sleep(0.5)
            
            print(f"\nğŸ“Š Káº¾T QUáº¢ Cáº¬P NHáº¬T:")
            print(f"âœ… ÄÃ£ thÃªm má»›i: {added_count} models")
            print(f"ğŸ”„ ÄÃ£ tá»“n táº¡i: {updated_count} models")
            
        except Exception as e:
            print(f"âŒ Lá»—i khi cáº­p nháº­t models: {e}")
    
    def test_model(self, model_name: str = None) -> None:
        """
        ğŸ§ª CHá»¨C NÄ‚NG 3: Test model
        """
        if not model_name:
            # Hiá»ƒn thá»‹ danh sÃ¡ch models Ä‘á»ƒ chá»n
            current_models = self.get_current_litellm_models()
            if not current_models:
                print("âŒ KhÃ´ng cÃ³ models nÃ o trong container!")
                return
            
            print("ğŸ“Š DANH SÃCH MODELS HIá»†N Táº I:")
            for i, model in enumerate(current_models, 1):
                print(f" {i}. {model}")
            
            try:
                choice = input("\nNháº­p sá»‘ thá»© tá»± model Ä‘á»ƒ test (hoáº·c nháº­p tÃªn model): ").strip()
                if choice.isdigit():
                    idx = int(choice) - 1
                    if 0 <= idx < len(current_models):
                        model_name = current_models[idx]
                    else:
                        print("âŒ Sá»‘ thá»© tá»± khÃ´ng há»£p lá»‡!")
                        return
                else:
                    model_name = choice
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ÄÃ£ há»§y test!")
                return
        
        print(f"ğŸ§ª Äang test model: {model_name}")
        
        # Test cÆ¡ báº£n
        test_payload = {
            "model": model_name,
            "messages": [
                {
                    "role": "user", 
                    "content": "Hello! Please respond with 'I am working correctly via shareapiai.com'"
                }
            ],
            "max_tokens": 50,
            "temperature": 0.1
        }
        
        try:
            start_time = time.time()
            response = requests.post(
                f"{self.litellm_base_url}/v1/chat/completions",
                headers=self.litellm_headers,
                json=test_payload,
                timeout=60
            )
            end_time = time.time()
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
                usage = result.get('usage', {})
                
                print(f"âœ… Model {model_name} hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng!")
                print(f"ğŸ“ Pháº£n há»“i: {content}")
                print(f"â±ï¸ Thá»i gian pháº£n há»“i: {end_time - start_time:.2f}s")
                print(f"ğŸ”¢ Tokens sá»­ dá»¥ng: {usage.get('total_tokens', 'N/A')}")
                
                # Test function calling náº¿u há»— trá»£
                self._test_function_calling(model_name)
                
            else:
                print(f"âŒ Model {model_name} lá»—i: {response.status_code}")
                print(f"ğŸ“„ Chi tiáº¿t: {response.text}")
                
        except Exception as e:
            print(f"âŒ Lá»—i khi test model {model_name}: {e}")
    
    def _test_function_calling(self, model_name: str) -> None:
        """Test function calling capability"""
        print(f"ğŸ”§ Äang test function calling cho {model_name}...")
        
        test_payload = {
            "model": model_name,
            "messages": [
                {"role": "user", "content": "What's the weather like in Hanoi?"}
            ],
            "functions": [
                {
                    "name": "get_weather",
                    "description": "Get current weather information for a city",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {
                                "type": "string",
                                "description": "The city name"
                            }
                        },
                        "required": ["location"]
                    }
                }
            ],
            "max_tokens": 100
        }
        
        try:
            response = requests.post(
                f"{self.litellm_base_url}/v1/chat/completions",
                headers=self.litellm_headers,
                json=test_payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                message = result['choices'][0]['message']
                
                if 'function_call' in message:
                    print(f"âœ… Function calling hoáº¡t Ä‘á»™ng!")
                    print(f"ğŸ”§ Function: {message['function_call']['name']}")
                else:
                    print(f"âš ï¸ Function calling khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng (cÃ³ thá»ƒ model khÃ´ng há»— trá»£)")
            else:
                print(f"âš ï¸ Function calling test tháº¥t báº¡i: {response.status_code}")
                
        except Exception as e:
            print(f"âš ï¸ KhÃ´ng thá»ƒ test function calling: {e}")
    
    def delete_model(self, model_name: str = None) -> None:
        """
        ğŸ—‘ï¸ CHá»¨C NÄ‚NG 4: XÃ³a model
        """
        if not model_name:
            # Hiá»ƒn thá»‹ danh sÃ¡ch models Ä‘á»ƒ chá»n
            current_models = self.get_current_litellm_models()
            if not current_models:
                print("âŒ KhÃ´ng cÃ³ models nÃ o trong container!")
                return
            
            print("ğŸ“Š DANH SÃCH MODELS HIá»†N Táº I:")
            for i, model in enumerate(current_models, 1):
                print(f" {i}. {model}")
            
            try:
                choice = input("\nNháº­p sá»‘ thá»© tá»± model Ä‘á»ƒ xÃ³a (hoáº·c nháº­p tÃªn model): ").strip()
                if choice.isdigit():
                    idx = int(choice) - 1
                    if 0 <= idx < len(current_models):
                        model_name = current_models[idx]
                    else:
                        print("âŒ Sá»‘ thá»© tá»± khÃ´ng há»£p lá»‡!")
                        return
                else:
                    model_name = choice
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ÄÃ£ há»§y xÃ³a!")
                return
        
        # XÃ¡c nháº­n xÃ³a
        confirm = input(f"âš ï¸ Báº¡n cÃ³ cháº¯c muá»‘n xÃ³a model '{model_name}'? (y/N): ").strip().lower()
        if confirm != 'y':
            print("ğŸ‘‹ ÄÃ£ há»§y xÃ³a!")
            return
        
        print(f"ğŸ—‘ï¸ Äang thá»­ xÃ³a model: {model_name}")
        
        # Thá»­ cÃ¡c phÆ°Æ¡ng phÃ¡p xÃ³a model
        success = False
        
        # PhÆ°Æ¡ng phÃ¡p 1: DELETE /model/{model_name}
        try:
            print("ğŸ”„ PhÆ°Æ¡ng phÃ¡p 1: DELETE /model/{model_name}")
            response = requests.delete(
                f"{self.litellm_base_url}/model/{model_name}",
                headers=self.litellm_headers,
                timeout=30
            )
            
            if response.status_code == 200:
                print(f"âœ… ÄÃ£ xÃ³a model {model_name} thÃ nh cÃ´ng!")
                success = True
            elif response.status_code == 404:
                print(f"âš ï¸ API tráº£ vá» 404 - Model khÃ´ng tá»“n táº¡i hoáº·c khÃ´ng thá»ƒ xÃ³a")
            elif response.status_code == 405:
                print(f"âš ï¸ Method khÃ´ng Ä‘Æ°á»£c há»— trá»£")
            else:
                print(f"âš ï¸ API tráº£ vá» status: {response.status_code}")
                print(f"ğŸ“„ Response: {response.text[:200]}...")
                
        except Exception as e:
            print(f"âŒ Lá»—i khi gá»i DELETE API: {e}")
        
        if not success:
            # PhÆ°Æ¡ng phÃ¡p 2: POST /model/delete
            try:
                print("ğŸ”„ PhÆ°Æ¡ng phÃ¡p 2: POST /model/delete")
                response = requests.post(
                    f"{self.litellm_base_url}/model/delete",
                    headers=self.litellm_headers,
                    json={"model_name": model_name},
                    timeout=30
                )
                
                if response.status_code == 200:
                    print(f"âœ… ÄÃ£ xÃ³a model {model_name} thÃ nh cÃ´ng!")
                    success = True
                else:
                    print(f"âš ï¸ POST delete tráº£ vá» status: {response.status_code}")
                    
            except Exception as e:
                print(f"âŒ Lá»—i khi gá»i POST delete: {e}")
        
        if not success:
            # PhÆ°Æ¡ng phÃ¡p 3: PUT /model/update vá»›i action delete
            try:
                print("ğŸ”„ PhÆ°Æ¡ng phÃ¡p 3: PUT /model/update")
                response = requests.put(
                    f"{self.litellm_base_url}/model/update",
                    headers=self.litellm_headers,
                    json={"model_name": model_name, "action": "delete"},
                    timeout=30
                )
                
                if response.status_code == 200:
                    print(f"âœ… ÄÃ£ xÃ³a model {model_name} thÃ nh cÃ´ng!")
                    success = True
                else:
                    print(f"âš ï¸ PUT update tráº£ vá» status: {response.status_code}")
                    
            except Exception as e:
                print(f"âŒ Lá»—i khi gá»i PUT update: {e}")
        
        if not success:
            print(f"\nâŒ KHÃ”NG THá»‚ XÃ“A MODEL QUA API")
            print(f"ğŸ” LiteLLM cÃ³ thá»ƒ khÃ´ng há»— trá»£ xÃ³a models qua REST API")
            
        # HÆ°á»›ng dáº«n xÃ³a thá»§ cÃ´ng
        print(f"\nğŸ“‹ HÆ¯á»šNG DáºªN XÃ“A MODEL THá»¦ CÃ”NG:")
        print(f"1. ğŸ”„ Restart container Ä‘á»ƒ reset táº¥t cáº£ models:")
        print(f"   az containerapp revision restart --name litellm-app --resource-group rg-litellm")
        print(f"2. ğŸ›‘ Stop vÃ  Start láº¡i container:")
        print(f"   az containerapp stop --name litellm-app --resource-group rg-litellm")
        print(f"   az containerapp start --name litellm-app --resource-group rg-litellm")
        print(f"3. ğŸŒ Qua Azure Portal:")
        print(f"   Container Apps â†’ litellm-app â†’ Restart")
        print(f"4. â• Sau Ä‘Ã³ cháº¡y láº¡i chá»©c nÄƒng 2 Ä‘á»ƒ thÃªm láº¡i models cáº§n thiáº¿t")
        
        # Kiá»ƒm tra model cÃ²n tá»“n táº¡i khÃ´ng
        print(f"\nğŸ” Kiá»ƒm tra model sau khi thá»­ xÃ³a...")
        remaining_models = self.get_current_litellm_models()
        if model_name in remaining_models:
            print(f"âš ï¸ Model {model_name} váº«n cÃ²n trong container")
        else:
            print(f"âœ… Model {model_name} Ä‘Ã£ khÃ´ng cÃ²n trong danh sÃ¡ch!")
    
    def delete_all_models(self) -> None:
        """
        ğŸ—‘ï¸ CHá»¨C NÄ‚NG 5: XÃ³a toÃ n bá»™ models
        """
        print("ğŸ—‘ï¸ XÃ“A TOÃ€N Bá»˜ MODELS")
        print("="*50)
        
        # Láº¥y danh sÃ¡ch models hiá»‡n táº¡i
        current_models = self.get_current_litellm_models()
        if not current_models:
            print("âŒ KhÃ´ng cÃ³ models nÃ o trong container!")
            return
        
        print(f"ğŸ“Š Hiá»‡n táº¡i cÃ³ {len(current_models)} models trong container:")
        for i, model in enumerate(current_models[:10], 1):
            print(f"  {i}. {model}")
        if len(current_models) > 10:
            print(f"  ... vÃ  {len(current_models) - 10} models khÃ¡c")
        
        print("\nâš ï¸  Cáº¢NH BÃO: Báº¡n sáº¯p xÃ³a TOÃ€N Bá»˜ models!")
        print("âš ï¸  HÃ nh Ä‘á»™ng nÃ y KHÃ”NG THá»‚ HOÃ€N TÃC!")
        print("âš ï¸  Container sáº½ khÃ´ng cÃ³ models nÃ o sau khi xÃ³a!")
        
        # XÃ¡c nháº­n láº§n 1
        confirm1 = input("\nğŸ¤” Báº¡n cÃ³ cháº¯c muá»‘n xÃ³a TOÃ€N Bá»˜ models? (yes/NO): ").strip()
        if confirm1.lower() != 'yes':
            print("ğŸ‘‹ ÄÃ£ há»§y xÃ³a toÃ n bá»™ models!")
            return
        
        # XÃ¡c nháº­n láº§n 2 vá»›i captcha
        captcha = random.randint(1000, 9999)
        print(f"\nğŸ” Äá»ƒ xÃ¡c nháº­n, vui lÃ²ng nháº­p sá»‘: {captcha}")
        captcha_input = input("Nháº­p sá»‘ xÃ¡c nháº­n: ").strip()
        
        if captcha_input != str(captcha):
            print("âŒ Sá»‘ xÃ¡c nháº­n khÃ´ng Ä‘Ãºng! ÄÃ£ há»§y xÃ³a.")
            return
        
        print("\nğŸ—‘ï¸ Báº¯t Ä‘áº§u xÃ³a toÃ n bá»™ models...")
        
        # PhÆ°Æ¡ng phÃ¡p 1: Thá»­ xÃ³a tá»«ng model qua API
        deleted_count = 0
        failed_count = 0
        
        for i, model in enumerate(current_models, 1):
            print(f"ğŸ—‘ï¸ Äang xÃ³a {i}/{len(current_models)}: {model}")
            
            try:
                # Thá»­ DELETE request
                response = requests.delete(
                    f"{self.litellm_base_url}/model/{model}",
                    headers=self.litellm_headers,
                    timeout=30
                )
                
                if response.status_code == 200:
                    print(f"  âœ… ÄÃ£ xÃ³a: {model}")
                    deleted_count += 1
                elif response.status_code == 404:
                    print(f"  âš ï¸ KhÃ´ng tá»“n táº¡i: {model}")
                    deleted_count += 1
                elif response.status_code == 405:
                    print(f"  âš ï¸ API khÃ´ng há»— trá»£ xÃ³a: {model}")
                    failed_count += 1
                else:
                    print(f"  âŒ Lá»—i {response.status_code}: {model}")
                    failed_count += 1
                    
            except Exception as e:
                print(f"  âŒ Lá»—i khi xÃ³a {model}: {e}")
                failed_count += 1
            
            # Delay Ä‘á»ƒ trÃ¡nh rate limit
            time.sleep(0.2)
        
        print(f"\nğŸ“Š Káº¾T QUáº¢ XÃ“A TOÃ€N Bá»˜ MODELS:")
        print(f"âœ… ÄÃ£ xÃ³a: {deleted_count} models")
        print(f"âŒ Tháº¥t báº¡i: {failed_count} models")
        
        if failed_count > 0:
            print(f"\nğŸ“‹ HÆ¯á»šNG DáºªN XÃ“A TOÃ€N Bá»˜ MODELS THá»¦ CÃ”NG:")
            print(f"1. Restart container Ä‘á»ƒ reset táº¥t cáº£ models:")
            print(f"   az containerapp revision restart --name litellm-app --resource-group rg-litellm")
            print(f"2. Hoáº·c stop vÃ  start láº¡i container:")
            print(f"   az containerapp stop --name litellm-app --resource-group rg-litellm")
            print(f"   az containerapp start --name litellm-app --resource-group rg-litellm")
            print(f"3. Container sáº½ khá»Ÿi Ä‘á»™ng vá»›i 0 models")
            print(f"4. Cháº¡y chá»©c nÄƒng 2 Ä‘á»ƒ thÃªm láº¡i models cáº§n thiáº¿t")
        
        # Kiá»ƒm tra láº¡i sau khi xÃ³a
        print(f"\nğŸ” Kiá»ƒm tra láº¡i models cÃ²n láº¡i...")
        remaining_models = self.get_current_litellm_models()
        if remaining_models:
            print(f"âš ï¸ CÃ²n láº¡i {len(remaining_models)} models:")
            for model in remaining_models[:5]:
                print(f"  - {model}")
            if len(remaining_models) > 5:
                print(f"  ... vÃ  {len(remaining_models) - 5} models khÃ¡c")
        else:
            print(f"âœ… Container Ä‘Ã£ sáº¡ch, khÃ´ng cÃ²n models nÃ o!")


def main():
    manager = ShareAPIAIModelManager()
    
    while True:
        print("\n" + "="*60)
        print("ğŸš€ SHAREAPIAI.COM - ADVANCED MODEL MANAGER")
        print("="*60)
        print("1. ğŸ“¥ Láº¥y models má»›i nháº¥t tá»« AIMLAPI vÃ  lÆ°u file")
        print("2. ğŸ”„ Cáº­p nháº­t models tá»« file vÃ o container")
        print("3. ğŸ§ª Test model")
        print("4. ğŸ—‘ï¸ XÃ³a model")
        print("5. ğŸ§¹ XÃ³a toÃ n bá»™ models")
        print("6. ğŸ“Š Xem thÃ´ng tin file models")
        print("7. ğŸ“‹ Xem models hiá»‡n táº¡i trong container")
        print("0. ğŸ‘‹ ThoÃ¡t")
        print("="*60)
        
        try:
            choice = input("Chá»n chá»©c nÄƒng (0-7): ").strip()
            
            if choice == "0":
                print("ğŸ‘‹ Táº¡m biá»‡t!")
                break
            elif choice == "1":
                print("\nğŸ”„ CHá»¨C NÄ‚NG 1: Láº¤Y MODELS Tá»ª AIMLAPI")
                models = manager.fetch_latest_models_from_aimlapi()
                if models:
                    print(f"âœ… ÄÃ£ láº¥y vÃ  lÆ°u {len(models)} models!")
            
            elif choice == "2":
                print("\nğŸ”„ CHá»¨C NÄ‚NG 2: Cáº¬P NHáº¬T MODELS VÃ€O CONTAINER")
                manager.update_models_to_container()
            
            elif choice == "3":
                print("\nğŸ§ª CHá»¨C NÄ‚NG 3: TEST MODEL")
                manager.test_model()
            
            elif choice == "4":
                print("\nğŸ—‘ï¸ CHá»¨C NÄ‚NG 4: XÃ“A MODEL")
                manager.delete_model()
            
            elif choice == "5":
                print("\nğŸ§¹ CHá»¨C NÄ‚NG 5: XÃ“A TOÃ€N Bá»˜ MODELS")
                manager.delete_all_models()
            
            elif choice == "6":
                print("\nğŸ“Š THÃ”NG TIN FILE MODELS:")
                if os.path.exists(manager.models_file):
                    with open(manager.models_file, 'r', encoding='utf-8') as f:
                        models_info = json.load(f)
                    print(f"ğŸ“… Cáº­p nháº­t láº§n cuá»‘i: {models_info.get('updated_at', 'N/A')}")
                    print(f"ğŸ”— Nguá»“n: {models_info.get('source', 'N/A')}")
                    print(f"ğŸ·ï¸ Enhanced by: {models_info.get('enhanced_by', 'N/A')}")
                    print(f"ğŸ“Š Tá»•ng models: {models_info.get('total_models', 0)}")
                else:
                    print("âŒ File models chÆ°a tá»“n táº¡i. Cháº¡y chá»©c nÄƒng 1 trÆ°á»›c!")
            
            elif choice == "7":
                print("\nğŸ“‹ MODELS HIá»†N Táº I TRONG CONTAINER:")
                current_models = manager.get_current_litellm_models()
                if current_models:
                    for i, model in enumerate(current_models, 1):
                        print(f" {i}. {model}")
                    print(f"\nğŸ“Š Tá»•ng: {len(current_models)} models")
                else:
                    print("âŒ KhÃ´ng cÃ³ models nÃ o trong container!")
            
            else:
                print("âŒ Lá»±a chá»n khÃ´ng há»£p lá»‡!")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Táº¡m biá»‡t!")
            break
        except Exception as e:
            print(f"âŒ Lá»—i: {e}")

if __name__ == "__main__":
    main()
