#!/usr/bin/env python3
"""
üöÄ SHAREAPIAI.COM - ADVANCED MODEL MANAGER
Qu·∫£n l√Ω models t·ª´ AIMLAPI v√† LiteLLM Container
"""

import requests
import json
import os
import time
import random
from datetime import datetime
from typing import List, Dict, Any
import yaml

class ShareAPIAIModelManager:
    def __init__(self):
        self.aimlapi_key = "b0197edcd9104cd1ab78aaf148ce609a"
        self.litellm_base_url = "https://call.shareapiai.com"
        self.litellm_api_key = "sk-hWv1u2fX3yG4zJ5kT6p7qR8sT9uV0wX1"
        self.models_file = "shareapiai_models_info.json"
        
        # Headers for AIMLAPI
        self.aimlapi_headers = {
            "Authorization": f"Bearer {self.aimlapi_key}",
            "Content-Type": "application/json"
        }
        
        # Headers for LiteLLM
        self.litellm_headers = {
            "Authorization": f"Bearer {self.litellm_api_key}",
            "Content-Type": "application/json"
        }
    
    def fetch_latest_models_from_aimlapi(self) -> List[Dict]:
        """
        üîÑ CH·ª®C NƒÇNG 1: L·∫•y th√¥ng tin models m·ªõi nh·∫•t t·ª´ AIMLAPI
        """
        print("üîÑ ƒêang l·∫•y danh s√°ch models m·ªõi nh·∫•t t·ª´ AIMLAPI...")
        
        try:
            response = requests.get(
                "https://api.aimlapi.com/models",
                headers=self.aimlapi_headers,
                timeout=30
            )
            response.raise_for_status()
            
            models_data = response.json()
            models = models_data.get('data', [])
            
            # Th√™m th√¥ng tin shareapiai.com v√†o m·ªói model
            enhanced_models = []
            for model in models:
                enhanced_model = {
                    "id": model.get('id', ''),
                    "object": model.get('object', 'model'),
                    "created": model.get('created', int(time.time())),
                    "owned_by": model.get('owned_by', 'aimlapi'),
                    "description": f"{model.get('id', '')} - Provided by shareapiai.com ",
                    "provider": "shareapiai.com",
                    "shareapiai_enhanced": True,
                    "shareapiai_added_date": datetime.now().isoformat(),
                    "api_base": "https://api.aimlapi.com/v1",
                    "max_tokens": self._get_model_max_tokens(model.get('id', '')),
                    "supports_function_calling": self._supports_function_calling(model.get('id', '')),
                    "supports_vision": self._supports_vision(model.get('id', '')),
                    "model_type": self._get_model_type(model.get('id', ''))
                }
                enhanced_models.append(enhanced_model)
            
            # L∆∞u v√†o file
            models_info = {
                "updated_at": datetime.now().isoformat(),
                "source": "https://api.aimlapi.com/models",
                "enhanced_by": "shareapiai.com",
                "total_models": len(enhanced_models),
                "models": enhanced_models
            }
            
            with open(self.models_file, 'w', encoding='utf-8') as f:
                json.dump(models_info, f, indent=2, ensure_ascii=False)
            
            print(f"‚úÖ ƒê√£ l∆∞u {len(enhanced_models)} models v√†o {self.models_file}")
            print(f"üìÅ File ƒë∆∞·ª£c c·∫≠p nh·∫≠t l√∫c: {models_info['updated_at']}")
            
            return enhanced_models
            
        except requests.exceptions.RequestException as e:
            print(f"‚ùå L·ªói khi l·∫•y models t·ª´ AIMLAPI: {e}")
            return []
        except Exception as e:
            print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
            return []
    
    def _get_model_max_tokens(self, model_id: str) -> int:
        """∆Ø·ªõc t√≠nh max tokens d·ª±a tr√™n model name"""
        model_id = model_id.lower()
        if 'gpt-4' in model_id:
            if 'turbo' in model_id or '1106' in model_id or '0125' in model_id:
                return 128000
            return 8192
        elif 'gpt-3.5' in model_id:
            if 'turbo' in model_id and ('1106' in model_id or '0125' in model_id):
                return 16385
            return 4096
        elif 'claude' in model_id:
            if 'claude-3' in model_id:
                return 200000
            return 100000
        elif 'gemini' in model_id:
            if '1.5' in model_id:
                return 2097152
            return 32768
        elif 'llama' in model_id:
            if '70b' in model_id or '405b' in model_id:
                return 128000
            return 8192
        else:
            return 4096
    
    def _supports_function_calling(self, model_id: str) -> bool:
        """Ki·ªÉm tra model c√≥ h·ªó tr·ª£ function calling kh√¥ng"""
        model_id = model_id.lower()
        function_calling_models = [
            'gpt-4', 'gpt-3.5-turbo', 'claude-3', 'gemini-1.5', 
            'llama-3.1', 'mistral', 'command-r'
        ]
        return any(model_name in model_id for model_name in function_calling_models)
    
    def _supports_vision(self, model_id: str) -> bool:
        """Ki·ªÉm tra model c√≥ h·ªó tr·ª£ vision kh√¥ng"""
        model_id = model_id.lower()
        vision_models = [
            'gpt-4o', 'gpt-4-vision', 'claude-3', 'gemini-1.5-pro',
            'llava', 'qwen-vl'
        ]
        return any(model_name in model_id for model_name in vision_models)
    
    def _get_model_type(self, model_id: str) -> str:
        """X√°c ƒë·ªãnh lo·∫°i model"""
        model_id = model_id.lower()
        if any(x in model_id for x in ['gpt', 'openai']):
            return 'openai'
        elif 'claude' in model_id:
            return 'anthropic'
        elif 'gemini' in model_id:
            return 'google'
        elif 'llama' in model_id:
            return 'meta'
        elif 'mistral' in model_id:
            return 'mistral'
        elif 'command' in model_id:
            return 'cohere'
        else:
            return 'other'
    
    def get_current_litellm_models(self) -> List[str]:
        """L·∫•y danh s√°ch models hi·ªán t·∫°i trong LiteLLM"""
        try:
            response = requests.get(
                f"{self.litellm_base_url}/v1/models",
                headers=self.litellm_headers,
                timeout=30
            )
            response.raise_for_status()
            
            models_data = response.json()
            return [model['id'] for model in models_data.get('data', [])]
            
        except Exception as e:
            print(f"‚ùå L·ªói khi l·∫•y models t·ª´ LiteLLM: {e}")
            return []
    
    def update_models_to_container(self) -> None:
        """
        üîÑ CH·ª®C NƒÇNG 2: C·∫≠p nh·∫≠t models t·ª´ file v√†o container
        """
        print("üîÑ ƒêang c·∫≠p nh·∫≠t models v√†o LiteLLM container...")
        
        # ƒê·ªçc file models
        if not os.path.exists(self.models_file):
            print(f"‚ùå File {self.models_file} kh√¥ng t·ªìn t·∫°i. Ch·∫°y ch·ª©c nƒÉng 1 tr∆∞·ªõc!")
            return
        
        try:
            with open(self.models_file, 'r', encoding='utf-8') as f:
                models_info = json.load(f)
            
            available_models = models_info.get('models', [])
            current_models = self.get_current_litellm_models()
            
            print(f"üìä C√≥ {len(available_models)} models trong file")
            print(f"üìä C√≥ {len(current_models)} models trong container")
            
            # Danh s√°ch models ph·ªï bi·∫øn ƒë·ªÉ ∆∞u ti√™n th√™m
            priority_models = [
                'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-3.5-turbo',
                'claude-3-5-sonnet', 'claude-3-opus', 'claude-3-haiku',
                'gemini-1.5-pro', 'gemini-1.5-flash',
                'llama-3.1-405b', 'llama-3.1-70b', 'llama-3.1-8b',
                'mistral-large', 'command-r-plus'
            ]
            
            added_count = 0
            updated_count = 0
            
            for model in available_models:
                model_id = model.get('id', '')
                
                # ∆Øu ti√™n models ph·ªï bi·∫øn
                is_priority = any(priority in model_id.lower() for priority in priority_models)
                
                if not is_priority and added_count >= 200:  # Gi·ªõi h·∫°n 20 models ƒë·ªÉ tr√°nh qu√° t·∫£i
                    continue
                
                if model_id in current_models:
                    print(f"üîÑ Model {model_id} ƒë√£ t·ªìn t·∫°i, b·ªè qua...")
                    updated_count += 1
                    continue
                
                # Th√™m model m·ªõi
                litellm_config = {
                    "model_name": model_id,
                    "litellm_params": {
                        "model": f"openai/{model_id}",
                        "api_base": model.get('api_base', 'https://api.aimlapi.com/v1'),
                        "api_key": "os.environ/AIMLAPI_KEY"
                    },
                    "model_info": {
                        "description": model.get('description', f"{model_id} via shareapiai.com"),
                        "provider": "shareapi",
                        "max_tokens": model.get('max_tokens', 4096),
                        "supports_function_calling": model.get('supports_function_calling', False),
                        "supports_vision": model.get('supports_vision', False),
                        "model_type": model.get('model_type', 'other'),
                        "shareapiai_enhanced": True
                    }
                }
                
                try:
                    response = requests.post(
                        f"{self.litellm_base_url}/model/new",
                        headers=self.litellm_headers,
                        json=litellm_config,
                        timeout=30
                    )
                    
                    if response.status_code == 200:
                        print(f"‚úÖ ƒê√£ th√™m model: {model_id}")
                        added_count += 1
                    else:
                        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ th√™m {model_id}: {response.status_code}")
                        
                except Exception as e:
                    print(f"‚ùå L·ªói khi th√™m {model_id}: {e}")
                
                # Delay ƒë·ªÉ tr√°nh rate limit
                time.sleep(0.5)
            
            print(f"\nüìä K·∫æT QU·∫¢ C·∫¨P NH·∫¨T:")
            print(f"‚úÖ ƒê√£ th√™m m·ªõi: {added_count} models")
            print(f"üîÑ ƒê√£ t·ªìn t·∫°i: {updated_count} models")
            
        except Exception as e:
            print(f"‚ùå L·ªói khi c·∫≠p nh·∫≠t models: {e}")
    
    def test_model(self, model_name: str = None) -> None:
        """
        üß™ CH·ª®C NƒÇNG 3: Test model
        """
        if not model_name:
            # Hi·ªÉn th·ªã danh s√°ch models ƒë·ªÉ ch·ªçn
            current_models = self.get_current_litellm_models()
            if not current_models:
                print("‚ùå Kh√¥ng c√≥ models n√†o trong container!")
                return
            
            print("üìä DANH S√ÅCH MODELS HI·ªÜN T·∫†I:")
            for i, model in enumerate(current_models, 1):
                print(f" {i}. {model}")
            
            try:
                choice = input("\nNh·∫≠p s·ªë th·ª© t·ª± model ƒë·ªÉ test (ho·∫∑c nh·∫≠p t√™n model): ").strip()
                if choice.isdigit():
                    idx = int(choice) - 1
                    if 0 <= idx < len(current_models):
                        model_name = current_models[idx]
                    else:
                        print("‚ùå S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá!")
                        return
                else:
                    model_name = choice
            except KeyboardInterrupt:
                print("\nüëã ƒê√£ h·ªßy test!")
                return
        
        print(f"üß™ ƒêang test model: {model_name}")
        
        # Test c∆° b·∫£n
        test_payload = {
            "model": model_name,
            "messages": [
                {
                    "role": "user", 
                    "content": "Hello! Please respond with 'I am working correctly via shareapiai.com'"
                }
            ],
            "max_tokens": 50,
            "temperature": 0.1
        }
        
        try:
            start_time = time.time()
            response = requests.post(
                f"{self.litellm_base_url}/v1/chat/completions",
                headers=self.litellm_headers,
                json=test_payload,
                timeout=60
            )
            end_time = time.time()
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
                usage = result.get('usage', {})
                
                print(f"‚úÖ Model {model_name} ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng!")
                print(f"üìù Ph·∫£n h·ªìi: {content}")
                print(f"‚è±Ô∏è Th·ªùi gian ph·∫£n h·ªìi: {end_time - start_time:.2f}s")
                print(f"üî¢ Tokens s·ª≠ d·ª•ng: {usage.get('total_tokens', 'N/A')}")
                
                # Test function calling n·∫øu h·ªó tr·ª£
                self._test_function_calling(model_name)
                
            else:
                print(f"‚ùå Model {model_name} l·ªói: {response.status_code}")
                print(f"üìÑ Chi ti·∫øt: {response.text}")
                
        except Exception as e:
            print(f"‚ùå L·ªói khi test model {model_name}: {e}")
    
    def _test_function_calling(self, model_name: str) -> None:
        """Test function calling capability"""
        print(f"üîß ƒêang test function calling cho {model_name}...")
        
        test_payload = {
            "model": model_name,
            "messages": [
                {"role": "user", "content": "What's the weather like in Hanoi?"}
            ],
            "functions": [
                {
                    "name": "get_weather",
                    "description": "Get current weather information for a city",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {
                                "type": "string",
                                "description": "The city name"
                            }
                        },
                        "required": ["location"]
                    }
                }
            ],
            "max_tokens": 100
        }
        
        try:
            response = requests.post(
                f"{self.litellm_base_url}/v1/chat/completions",
                headers=self.litellm_headers,
                json=test_payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                message = result['choices'][0]['message']
                
                if 'function_call' in message:
                    print(f"‚úÖ Function calling ho·∫°t ƒë·ªông!")
                    print(f"üîß Function: {message['function_call']['name']}")
                else:
                    print(f"‚ö†Ô∏è Function calling kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng (c√≥ th·ªÉ model kh√¥ng h·ªó tr·ª£)")
            else:
                print(f"‚ö†Ô∏è Function calling test th·∫•t b·∫°i: {response.status_code}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ test function calling: {e}")
    
    def delete_model(self, model_name: str = None) -> None:
        """
        üóëÔ∏è CH·ª®C NƒÇNG 4: X√≥a model
        """
        if not model_name:
            # Hi·ªÉn th·ªã danh s√°ch models ƒë·ªÉ ch·ªçn
            current_models = self.get_current_litellm_models()
            if not current_models:
                print("‚ùå Kh√¥ng c√≥ models n√†o trong container!")
                return
            
            print("üìä DANH S√ÅCH MODELS HI·ªÜN T·∫†I:")
            for i, model in enumerate(current_models, 1):
                print(f" {i}. {model}")
            
            try:
                choice = input("\nNh·∫≠p s·ªë th·ª© t·ª± model ƒë·ªÉ x√≥a (ho·∫∑c nh·∫≠p t√™n model): ").strip()
                if choice.isdigit():
                    idx = int(choice) - 1
                    if 0 <= idx < len(current_models):
                        model_name = current_models[idx]
                    else:
                        print("‚ùå S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá!")
                        return
                else:
                    model_name = choice
            except KeyboardInterrupt:
                print("\nüëã ƒê√£ h·ªßy x√≥a!")
                return
        
        # X√°c nh·∫≠n x√≥a
        confirm = input(f"‚ö†Ô∏è B·∫°n c√≥ ch·∫Øc mu·ªën x√≥a model '{model_name}'? (y/N): ").strip().lower()
        if confirm != 'y':
            print("üëã ƒê√£ h·ªßy x√≥a!")
            return
        
        print(f"üóëÔ∏è ƒêang th·ª≠ x√≥a model: {model_name}")
        
        # Th·ª≠ c√°c ph∆∞∆°ng ph√°p x√≥a model
        success = False
        
        # Ph∆∞∆°ng ph√°p 1: DELETE /model/{model_name}
        try:
            print("üîÑ Ph∆∞∆°ng ph√°p 1: DELETE /model/{model_name}")
            response = requests.delete(
                f"{self.litellm_base_url}/model/{model_name}",
                headers=self.litellm_headers,
                timeout=30
            )
            
            if response.status_code == 200:
                print(f"‚úÖ ƒê√£ x√≥a model {model_name} th√†nh c√¥ng!")
                success = True
            elif response.status_code == 404:
                print(f"‚ö†Ô∏è API tr·∫£ v·ªÅ 404 - Model kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng th·ªÉ x√≥a")
            elif response.status_code == 405:
                print(f"‚ö†Ô∏è Method kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£")
            else:
                print(f"‚ö†Ô∏è API tr·∫£ v·ªÅ status: {response.status_code}")
                print(f"üìÑ Response: {response.text[:200]}...")
                
        except Exception as e:
            print(f"‚ùå L·ªói khi g·ªçi DELETE API: {e}")
        
        if not success:
            # Ph∆∞∆°ng ph√°p 2: POST /model/delete
            try:
                print("üîÑ Ph∆∞∆°ng ph√°p 2: POST /model/delete")
                response = requests.post(
                    f"{self.litellm_base_url}/model/delete",
                    headers=self.litellm_headers,
                    json={"model_name": model_name},
                    timeout=30
                )
                
                if response.status_code == 200:
                    print(f"‚úÖ ƒê√£ x√≥a model {model_name} th√†nh c√¥ng!")
                    success = True
                else:
                    print(f"‚ö†Ô∏è POST delete tr·∫£ v·ªÅ status: {response.status_code}")
                    
            except Exception as e:
                print(f"‚ùå L·ªói khi g·ªçi POST delete: {e}")
        
        if not success:
            # Ph∆∞∆°ng ph√°p 3: PUT /model/update v·ªõi action delete
            try:
                print("üîÑ Ph∆∞∆°ng ph√°p 3: PUT /model/update")
                response = requests.put(
                    f"{self.litellm_base_url}/model/update",
                    headers=self.litellm_headers,
                    json={"model_name": model_name, "action": "delete"},
                    timeout=30
                )
                
                if response.status_code == 200:
                    print(f"‚úÖ ƒê√£ x√≥a model {model_name} th√†nh c√¥ng!")
                    success = True
                else:
                    print(f"‚ö†Ô∏è PUT update tr·∫£ v·ªÅ status: {response.status_code}")
                    
            except Exception as e:
                print(f"‚ùå L·ªói khi g·ªçi PUT update: {e}")
        
        if not success:
            print(f"\n‚ùå KH√îNG TH·ªÇ X√ìA MODEL QUA API")
            print(f"üîç LiteLLM c√≥ th·ªÉ kh√¥ng h·ªó tr·ª£ x√≥a models qua REST API")
            
        # H∆∞·ªõng d·∫´n x√≥a th·ªß c√¥ng
        print(f"\nüìã H∆Ø·ªöNG D·∫™N X√ìA MODEL TH·ª¶ C√îNG:")
        print(f"1. üîÑ Restart container ƒë·ªÉ reset t·∫•t c·∫£ models:")
        print(f"   az containerapp revision restart --name litellm-app --resource-group rg-litellm")
        print(f"2. üõë Stop v√† Start l·∫°i container:")
        print(f"   az containerapp stop --name litellm-app --resource-group rg-litellm")
        print(f"   az containerapp start --name litellm-app --resource-group rg-litellm")
        print(f"3. üåê Qua Azure Portal:")
        print(f"   Container Apps ‚Üí litellm-app ‚Üí Restart")
        print(f"4. ‚ûï Sau ƒë√≥ ch·∫°y l·∫°i ch·ª©c nƒÉng 2 ƒë·ªÉ th√™m l·∫°i models c·∫ßn thi·∫øt")
        
        # Ki·ªÉm tra model c√≤n t·ªìn t·∫°i kh√¥ng
        print(f"\nüîç Ki·ªÉm tra model sau khi th·ª≠ x√≥a...")
        remaining_models = self.get_current_litellm_models()
        if model_name in remaining_models:
            print(f"‚ö†Ô∏è Model {model_name} v·∫´n c√≤n trong container")
        else:
            print(f"‚úÖ Model {model_name} ƒë√£ kh√¥ng c√≤n trong danh s√°ch!")
    
    def delete_all_models(self) -> None:
        """
        üóëÔ∏è CH·ª®C NƒÇNG 5: X√≥a to√†n b·ªô models
        """
        print("üóëÔ∏è X√ìA TO√ÄN B·ªò MODELS")
        print("="*50)
        
        # L·∫•y danh s√°ch models hi·ªán t·∫°i
        current_models = self.get_current_litellm_models()
        if not current_models:
            print("‚ùå Kh√¥ng c√≥ models n√†o trong container!")
            return
        
        print(f"üìä Hi·ªán t·∫°i c√≥ {len(current_models)} models trong container:")
        for i, model in enumerate(current_models[:10], 1):
            print(f"  {i}. {model}")
        if len(current_models) > 10:
            print(f"  ... v√† {len(current_models) - 10} models kh√°c")
        
        print("\n‚ö†Ô∏è  C·∫¢NH B√ÅO: B·∫°n s·∫Øp x√≥a TO√ÄN B·ªò models!")
        print("‚ö†Ô∏è  H√†nh ƒë·ªông n√†y KH√îNG TH·ªÇ HO√ÄN T√ÅC!")
        print("‚ö†Ô∏è  Container s·∫Ω kh√¥ng c√≥ models n√†o sau khi x√≥a!")
        
        # X√°c nh·∫≠n l·∫ßn 1
        confirm1 = input("\nü§î B·∫°n c√≥ ch·∫Øc mu·ªën x√≥a TO√ÄN B·ªò models? (yes/NO): ").strip()
        if confirm1.lower() != 'yes':
            print("üëã ƒê√£ h·ªßy x√≥a to√†n b·ªô models!")
            return
        
        # X√°c nh·∫≠n l·∫ßn 2 v·ªõi captcha
        captcha = random.randint(1000, 9999)
        print(f"\nüîê ƒê·ªÉ x√°c nh·∫≠n, vui l√≤ng nh·∫≠p s·ªë: {captcha}")
        captcha_input = input("Nh·∫≠p s·ªë x√°c nh·∫≠n: ").strip()
        
        if captcha_input != str(captcha):
            print("‚ùå S·ªë x√°c nh·∫≠n kh√¥ng ƒë√∫ng! ƒê√£ h·ªßy x√≥a.")
            return
        
        print("\nüóëÔ∏è B·∫Øt ƒë·∫ßu x√≥a to√†n b·ªô models...")
        
        # Ph∆∞∆°ng ph√°p 1: Th·ª≠ x√≥a t·ª´ng model qua API
        deleted_count = 0
        failed_count = 0
        
        for i, model in enumerate(current_models, 1):
            print(f"üóëÔ∏è ƒêang x√≥a {i}/{len(current_models)}: {model}")
            
            try:
                # Th·ª≠ DELETE request
                response = requests.delete(
                    f"{self.litellm_base_url}/model/{model}",
                    headers=self.litellm_headers,
                    timeout=30
                )
                
                if response.status_code == 200:
                    print(f"  ‚úÖ ƒê√£ x√≥a: {model}")
                    deleted_count += 1
                elif response.status_code == 404:
                    print(f"  ‚ö†Ô∏è Kh√¥ng t·ªìn t·∫°i: {model}")
                    deleted_count += 1
                elif response.status_code == 405:
                    print(f"  ‚ö†Ô∏è API kh√¥ng h·ªó tr·ª£ x√≥a: {model}")
                    failed_count += 1
                else:
                    print(f"  ‚ùå L·ªói {response.status_code}: {model}")
                    failed_count += 1
                    
            except Exception as e:
                print(f"  ‚ùå L·ªói khi x√≥a {model}: {e}")
                failed_count += 1
            
            # Delay ƒë·ªÉ tr√°nh rate limit
            time.sleep(0.2)
        
        print(f"\nüìä K·∫æT QU·∫¢ X√ìA TO√ÄN B·ªò MODELS:")
        print(f"‚úÖ ƒê√£ x√≥a: {deleted_count} models")
        print(f"‚ùå Th·∫•t b·∫°i: {failed_count} models")
        
        if failed_count > 0:
            print(f"\nüìã H∆Ø·ªöNG D·∫™N X√ìA TO√ÄN B·ªò MODELS TH·ª¶ C√îNG:")
            print(f"1. Restart container ƒë·ªÉ reset t·∫•t c·∫£ models:")
            print(f"   az containerapp revision restart --name litellm-app --resource-group rg-litellm")
            print(f"2. Ho·∫∑c stop v√† start l·∫°i container:")
            print(f"   az containerapp stop --name litellm-app --resource-group rg-litellm")
            print(f"   az containerapp start --name litellm-app --resource-group rg-litellm")
            print(f"3. Container s·∫Ω kh·ªüi ƒë·ªông v·ªõi 0 models")
            print(f"4. Ch·∫°y ch·ª©c nƒÉng 2 ƒë·ªÉ th√™m l·∫°i models c·∫ßn thi·∫øt")
        
        # Ki·ªÉm tra l·∫°i sau khi x√≥a
        print(f"\nüîç Ki·ªÉm tra l·∫°i models c√≤n l·∫°i...")
        remaining_models = self.get_current_litellm_models()
        if remaining_models:
            print(f"‚ö†Ô∏è C√≤n l·∫°i {len(remaining_models)} models:")
            for model in remaining_models[:5]:
                print(f"  - {model}")
            if len(remaining_models) > 5:
                print(f"  ... v√† {len(remaining_models) - 5} models kh√°c")
        else:
            print(f"‚úÖ Container ƒë√£ s·∫°ch, kh√¥ng c√≤n models n√†o!")


def main():
    manager = ShareAPIAIModelManager()
    
    while True:
        print("\n" + "="*60)
        print("üöÄ SHAREAPIAI.COM - ADVANCED MODEL MANAGER")
        print("="*60)
        print("1. üì• L·∫•y models m·ªõi nh·∫•t t·ª´ AIMLAPI v√† l∆∞u file")
        print("2. üîÑ C·∫≠p nh·∫≠t models t·ª´ file v√†o container")
        print("3. üß™ Test model")
        print("4. üóëÔ∏è X√≥a model")
        print("5. üßπ X√≥a to√†n b·ªô models")
        print("6. üìä Xem th√¥ng tin file models")
        print("7. üìã Xem models hi·ªán t·∫°i trong container")
        print("0. üëã Tho√°t")
        print("="*60)
        
        try:
            choice = input("Ch·ªçn ch·ª©c nƒÉng (0-7): ").strip()
            
            if choice == "0":
                print("üëã T·∫°m bi·ªát!")
                break
            elif choice == "1":
                print("\nüîÑ CH·ª®C NƒÇNG 1: L·∫§Y MODELS T·ª™ AIMLAPI")
                models = manager.fetch_latest_models_from_aimlapi()
                if models:
                    print(f"‚úÖ ƒê√£ l·∫•y v√† l∆∞u {len(models)} models!")
            
            elif choice == "2":
                print("\nüîÑ CH·ª®C NƒÇNG 2: C·∫¨P NH·∫¨T MODELS V√ÄO CONTAINER")
                manager.update_models_to_container()
            
            elif choice == "3":
                print("\nüß™ CH·ª®C NƒÇNG 3: TEST MODEL")
                manager.test_model()
            
            elif choice == "4":
                print("\nüóëÔ∏è CH·ª®C NƒÇNG 4: X√ìA MODEL")
                manager.delete_model()
            
            elif choice == "5":
                print("\nüßπ CH·ª®C NƒÇNG 5: X√ìA TO√ÄN B·ªò MODELS")
                manager.delete_all_models()
            
            elif choice == "6":
                print("\nüìä TH√îNG TIN FILE MODELS:")
                if os.path.exists(manager.models_file):
                    with open(manager.models_file, 'r', encoding='utf-8') as f:
                        models_info = json.load(f)
                    print(f"üìÖ C·∫≠p nh·∫≠t l·∫ßn cu·ªëi: {models_info.get('updated_at', 'N/A')}")
                    print(f"üîó Ngu·ªìn: {models_info.get('source', 'N/A')}")
                    print(f"üè∑Ô∏è Enhanced by: {models_info.get('enhanced_by', 'N/A')}")
                    print(f"üìä T·ªïng models: {models_info.get('total_models', 0)}")
                else:
                    print("‚ùå File models ch∆∞a t·ªìn t·∫°i. Ch·∫°y ch·ª©c nƒÉng 1 tr∆∞·ªõc!")
            
            elif choice == "7":
                print("\nüìã MODELS HI·ªÜN T·∫†I TRONG CONTAINER:")
                current_models = manager.get_current_litellm_models()
                if current_models:
                    for i, model in enumerate(current_models, 1):
                        print(f" {i}. {model}")
                    print(f"\nüìä T·ªïng: {len(current_models)} models")
                else:
                    print("‚ùå Kh√¥ng c√≥ models n√†o trong container!")
            
            else:
                print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")
                
        except KeyboardInterrupt:
            print("\n\nüëã T·∫°m bi·ªát!")
            break
        except Exception as e:
            print(f"‚ùå L·ªói: {e}")

if __name__ == "__main__":
    main()
