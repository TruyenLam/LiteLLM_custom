#!/usr/bin/env python3
"""
AIMLAPI Models Fetcher and LiteLLM Configuration Updater
Láº¥y danh sÃ¡ch models tá»« AIMLAPI vÃ  cáº­p nháº­t vÃ o Azure Container App
"""

import os
import json
import requests
from typing import List, Dict, Any
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class AIMLAPIModelFetcher:
    def __init__(self):
        self.api_key = os.getenv('AIMLAPI_KEY')
        if not self.api_key:
            raise ValueError("AIMLAPI_KEY not found in environment variables")
        
        self.client = OpenAI(
            base_url="https://api.aimlapi.com/v1",
            api_key=self.api_key,
        )
        
        self.base_url = "https://api.aimlapi.com/v1"
        
    def get_available_models(self) -> List[Dict[str, Any]]:
        """Láº¥y danh sÃ¡ch táº¥t cáº£ models cÃ³ sáºµn tá»« AIMLAPI"""
        try:
            print("ğŸ” Äang láº¥y danh sÃ¡ch models tá»« AIMLAPI...")
            
            # Sá»­ dá»¥ng OpenAI client Ä‘á»ƒ láº¥y models
            models_response = self.client.models.list()
            
            models = []
            for model in models_response.data:
                model_info = {
                    "id": model.id,
                    "object": model.object,
                    "created": getattr(model, 'created', None),
                    "owned_by": getattr(model, 'owned_by', 'aimlapi')
                }
                models.append(model_info)
            
            print(f"âœ… TÃ¬m tháº¥y {len(models)} models")
            return models
            
        except Exception as e:
            print(f"âŒ Lá»—i khi láº¥y models: {e}")
            return []
    
    def test_model_connection(self, model_id: str) -> bool:
        """Test xem model cÃ³ hoáº¡t Ä‘á»™ng khÃ´ng"""
        try:
            response = self.client.chat.completions.create(
                model=model_id,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=5
            )
            return True
        except Exception as e:
            print(f"âš ï¸  Model {model_id} khÃ´ng hoáº¡t Ä‘á»™ng: {e}")
            return False
    
    def generate_litellm_config(self, models: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Táº¡o cáº¥u hÃ¬nh LiteLLM tá»« danh sÃ¡ch models"""
        
        # Lá»c vÃ  categorize models
        chat_models = []
        embedding_models = []
        
        for model in models:
            model_id = model['id']
            
            # Skip models that are likely not for chat completion
            skip_keywords = ['whisper', 'tts', 'dall-e', 'embedding']
            if any(keyword in model_id.lower() for keyword in skip_keywords):
                if 'embedding' in model_id.lower():
                    embedding_models.append(model_id)
                continue
            
            chat_models.append(model_id)
        
        # Táº¡o config cho LiteLLM
        litellm_config = {
            "model_list": [],
            "general_settings": {
                "master_key": "sk-hWv1u2fX3yG4zJ5kT6p7qR8sT9uV0wX1",
                "database_url": "postgresql://llmproxy:bTBwUQHt7VTltFyB@34.136.3.30:5432/litellm",
                "store_model_in_db": True
            }
        }
        
        # ThÃªm chat models
        for model_id in chat_models:
            model_config = {
                "model_name": model_id,
                "litellm_params": {
                    "model": f"openai/{model_id}",
                    "api_base": "https://api.aimlapi.com/v1",
                    "api_key": f"os.environ/AIMLAPI_KEY"
                }
            }
            litellm_config["model_list"].append(model_config)
        
        # ThÃªm embedding models
        for model_id in embedding_models:
            model_config = {
                "model_name": model_id,
                "litellm_params": {
                    "model": f"openai/{model_id}",
                    "api_base": "https://api.aimlapi.com/v1",
                    "api_key": f"os.environ/AIMLAPI_KEY"
                }
            }
            litellm_config["model_list"].append(model_config)
        
        return litellm_config
    
    def save_config_file(self, config: Dict[str, Any], filename: str = "aimlapi_config.yaml"):
        """LÆ°u cáº¥u hÃ¬nh ra file YAML"""
        import yaml
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
            print(f"âœ… ÄÃ£ lÆ°u cáº¥u hÃ¬nh vÃ o {filename}")
        except ImportError:
            # Fallback to JSON if PyYAML not available
            json_filename = filename.replace('.yaml', '.json')
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            print(f"âœ… ÄÃ£ lÆ°u cáº¥u hÃ¬nh vÃ o {json_filename}")
    
    def display_models_summary(self, models: List[Dict[str, Any]]):
        """Hiá»ƒn thá»‹ tÃ³m táº¯t models"""
        print("\n" + "="*60)
        print(f"ğŸ“Š TÃ“M Táº®T MODELS AIMLAPI ({len(models)} models)")
        print("="*60)
        
        # Group by type
        chat_models = []
        embedding_models = []
        other_models = []
        
        for model in models:
            model_id = model['id']
            if 'embedding' in model_id.lower():
                embedding_models.append(model_id)
            elif any(keyword in model_id.lower() for keyword in ['whisper', 'tts', 'dall-e']):
                other_models.append(model_id)
            else:
                chat_models.append(model_id)
        
        print(f"ğŸ’¬ Chat Models: {len(chat_models)}")
        for model in chat_models[:10]:  # Show first 10
            print(f"   - {model}")
        if len(chat_models) > 10:
            print(f"   ... vÃ  {len(chat_models) - 10} models khÃ¡c")
        
        print(f"\nğŸ”¤ Embedding Models: {len(embedding_models)}")
        for model in embedding_models:
            print(f"   - {model}")
        
        print(f"\nğŸµ Other Models: {len(other_models)}")
        for model in other_models:
            print(f"   - {model}")
        
        print("="*60)

def main():
    """Main function"""
    print("ğŸš€ AIMLAPI Models Fetcher for LiteLLM")
    print("="*50)
    
    try:
        # Initialize fetcher
        fetcher = AIMLAPIModelFetcher()
        
        # Get models
        models = fetcher.get_available_models()
        
        if not models:
            print("âŒ KhÃ´ng thá»ƒ láº¥y danh sÃ¡ch models")
            return
        
        # Display summary
        fetcher.display_models_summary(models)
        
        # Generate LiteLLM config
        print("\nğŸ”§ Äang táº¡o cáº¥u hÃ¬nh LiteLLM...")
        config = fetcher.generate_litellm_config(models)
        
        # Save config files
        fetcher.save_config_file(config, "aimlapi_litellm_config.yaml")
        
        # Save raw models data
        with open("aimlapi_models_raw.json", 'w', encoding='utf-8') as f:
            json.dump(models, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… HOÃ€N THÃ€NH!")
        print(f"ğŸ“ Files Ä‘Æ°á»£c táº¡o:")
        print(f"   - aimlapi_litellm_config.yaml (LiteLLM config)")
        print(f"   - aimlapi_models_raw.json (Raw models data)")
        
        print(f"\nğŸš€ BÆ¯á»šC TIáº¾P THEO:")
        print(f"   1. Review file config")
        print(f"   2. Upload config lÃªn Azure Container App")
        print(f"   3. Restart container vá»›i config má»›i")
        
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")

if __name__ == "__main__":
    main()
