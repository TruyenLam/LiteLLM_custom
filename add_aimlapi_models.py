#!/usr/bin/env python3
"""
Add AIMLAPI Models to LiteLLM via API
ThÃªm models AIMLAPI vÃ o LiteLLM container mÃ  khÃ´ng cáº§n restart
"""

import requests
import json
from typing import Dict, Any, List

class LiteLLMModelManager:
    def __init__(self, base_url: str = "https://call.shareapiai.com", api_key: str = "sk-hWv1u2fX3yG4zJ5kT6p7qR8sT9uV0wX1"):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    def get_current_models(self) -> List[Dict[str, Any]]:
        """Láº¥y danh sÃ¡ch models hiá»‡n táº¡i"""
        try:
            response = requests.get(f"{self.base_url}/v1/models", headers=self.headers)
            response.raise_for_status()
            return response.json().get("data", [])
        except Exception as e:
            print(f"âŒ Lá»—i khi láº¥y models: {e}")
            return []
    
    def get_model_info(self) -> List[Dict[str, Any]]:
        """Láº¥y thÃ´ng tin chi tiáº¿t models"""
        try:
            response = requests.get(f"{self.base_url}/model/info", headers=self.headers)
            response.raise_for_status()
            return response.json().get("data", [])
        except Exception as e:
            print(f"âŒ Lá»—i khi láº¥y model info: {e}")
            return []
    
    def add_model(self, model_config: Dict[str, Any]) -> bool:
        """ThÃªm model má»›i qua API"""
        try:
            response = requests.post(f"{self.base_url}/model/new", 
                                   headers=self.headers, 
                                   json=model_config)
            response.raise_for_status()
            print(f"âœ… ÄÃ£ thÃªm model: {model_config['model_name']}")
            return True
        except Exception as e:
            print(f"âŒ Lá»—i khi thÃªm model {model_config['model_name']}: {e}")
            if hasattr(e, 'response') and e.response:
                print(f"   Response: {e.response.text}")
            return False
    
    def add_aimlapi_models(self) -> bool:
        """ThÃªm cÃ¡c models AIMLAPI phá»• biáº¿n"""
        
        # Danh sÃ¡ch models AIMLAPI phá»• biáº¿n
        aimlapi_models = [
            {
                "model_name": "chatgpt-4o",
                "litellm_params": {
                    "model": "openai/gpt-4o",
                    "api_base": "https://api.aimlapi.com/v1",
                    "api_key": "os.environ/AIMLAPI_KEY"
                },
                "model_info": {
                    "description": "ChatGPT-4o model via AIMLAPI - Most capable model",
                    "provider": "aimlapi",
                    "max_tokens": 128000,
                    "supports_function_calling": True,
                    "supports_vision": True,
                    "cost_per_input_token": 0.0000025,
                    "cost_per_output_token": 0.00001
                }
            },
            {
                "model_name": "chatgpt-4o-mini",
                "litellm_params": {
                    "model": "openai/gpt-4o-mini",
                    "api_base": "https://api.aimlapi.com/v1",
                    "api_key": "os.environ/AIMLAPI_KEY"
                },
                "model_info": {
                    "description": "ChatGPT-4o-mini model via AIMLAPI - Cost effective",
                    "provider": "aimlapi",
                    "max_tokens": 128000,
                    "supports_function_calling": True,
                    "cost_per_input_token": 0.00000015,
                    "cost_per_output_token": 0.0000006
                }
            },
            {
                "model_name": "gpt-4-turbo",
                "litellm_params": {
                    "model": "openai/gpt-4-turbo",
                    "api_base": "https://api.aimlapi.com/v1",
                    "api_key": "os.environ/AIMLAPI_KEY"
                },
                "model_info": {
                    "description": "GPT-4 Turbo model via AIMLAPI - High performance",
                    "provider": "aimlapi",
                    "max_tokens": 128000,
                    "supports_function_calling": True,
                    "supports_vision": True,
                    "cost_per_input_token": 0.00001,
                    "cost_per_output_token": 0.00003
                }
            },
            {
                "model_name": "claude-3-5-sonnet",
                "litellm_params": {
                    "model": "openai/claude-3-5-sonnet-20241022",
                    "api_base": "https://api.aimlapi.com/v1",
                    "api_key": "os.environ/AIMLAPI_KEY"
                },
                "model_info": {
                    "description": "Claude 3.5 Sonnet via AIMLAPI - Excellent reasoning",
                    "provider": "aimlapi",
                    "max_tokens": 200000,
                    "supports_function_calling": True,
                    "cost_per_input_token": 0.000003,
                    "cost_per_output_token": 0.000015
                }
            },
            {
                "model_name": "gemini-1-5-pro",
                "litellm_params": {
                    "model": "openai/gemini-1.5-pro-latest",
                    "api_base": "https://api.aimlapi.com/v1",
                    "api_key": "os.environ/AIMLAPI_KEY"
                },
                "model_info": {
                    "description": "Gemini 1.5 Pro via AIMLAPI - Google's flagship model",
                    "provider": "aimlapi",
                    "max_tokens": 2097152,
                    "supports_function_calling": True,
                    "cost_per_input_token": 0.00000125,
                    "cost_per_output_token": 0.000005
                }
            }
        ]
        
        success_count = 0
        for model_config in aimlapi_models:
            if self.add_model(model_config):
                success_count += 1
        
        print(f"\nğŸ“Š ÄÃ£ thÃªm {success_count}/{len(aimlapi_models)} models")
        return success_count > 0
    
    def test_model(self, model_name: str) -> bool:
        """Test model cÃ³ hoáº¡t Ä‘á»™ng khÃ´ng"""
        try:
            test_payload = {
                "model": model_name,
                "messages": [{"role": "user", "content": "Hello! Please respond with 'OK' if you're working."}],
                "max_tokens": 10
            }
            
            response = requests.post(f"{self.base_url}/v1/chat/completions",
                                   headers=self.headers,
                                   json=test_payload)
            response.raise_for_status()
            
            result = response.json()
            if "choices" in result and len(result["choices"]) > 0:
                print(f"âœ… Model {model_name} hoáº¡t Ä‘á»™ng: {result['choices'][0]['message']['content']}")
                return True
            else:
                print(f"âŒ Model {model_name} khÃ´ng pháº£n há»“i Ä‘Ãºng")
                return False
                
        except Exception as e:
            print(f"âŒ Model {model_name} lá»—i: {e}")
            return False
    
    def display_models_summary(self, models: List[Dict[str, Any]]):
        """Hiá»ƒn thá»‹ tÃ³m táº¯t models"""
        print("\n" + "="*60)
        print(f"ğŸ“Š DANH SÃCH MODELS HIá»†N Táº I ({len(models)} models)")
        print("="*60)
        
        for i, model in enumerate(models, 1):
            model_id = model.get('id', 'unknown')
            print(f"{i:2d}. {model_id}")
        
        print("="*60)

def main():
    """Main function"""
    print("ğŸš€ LiteLLM Model Manager - Add AIMLAPI Models")
    print("="*60)
    
    manager = LiteLLMModelManager()
    
    # Kiá»ƒm tra káº¿t ná»‘i
    print("ğŸ” Kiá»ƒm tra káº¿t ná»‘i Ä‘áº¿n LiteLLM...")
    try:
        response = requests.get(f"{manager.base_url}/health/liveliness")
        if response.status_code == 200:
            print("âœ… LiteLLM Ä‘ang hoáº¡t Ä‘á»™ng")
        else:
            print(f"âŒ LiteLLM khÃ´ng pháº£n há»“i: {response.status_code}")
            return
    except Exception as e:
        print(f"âŒ KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n LiteLLM: {e}")
        return
    
    # Láº¥y models hiá»‡n táº¡i
    print("\nğŸ” Láº¥y danh sÃ¡ch models hiá»‡n táº¡i...")
    current_models = manager.get_current_models()
    manager.display_models_summary(current_models)
    
    # ThÃªm models AIMLAPI
    print("\nğŸ”§ ThÃªm models AIMLAPI...")
    success = manager.add_aimlapi_models()
    
    if success:
        print("\nâ³ Chá» models Ä‘Æ°á»£c cáº­p nháº­t...")
        import time
        time.sleep(5)
        
        # Láº¥y models sau khi thÃªm
        print("\nğŸ” Kiá»ƒm tra models sau khi thÃªm...")
        updated_models = manager.get_current_models()
        manager.display_models_summary(updated_models)
        
        # Test má»™t vÃ i models
        test_models = ["chatgpt-4o-mini", "gpt-4-turbo"]
        print(f"\nğŸ§ª Test models...")
        for model_name in test_models:
            if any(m.get('id') == model_name for m in updated_models):
                manager.test_model(model_name)
        
        print(f"\nğŸ‰ HOÃ€N THÃ€NH!")
        print(f"ğŸ“Š Tá»•ng models: {len(updated_models)}")
        print(f"ğŸŒ API URL: {manager.base_url}")
        print(f"ğŸ”‘ API Key: {manager.api_key[:20]}...")
        
        print(f"\nğŸš€ CÃCH Sá»¬ Dá»¤NG:")
        print(f"curl -X POST {manager.base_url}/v1/chat/completions \\")
        print(f'  -H "Authorization: Bearer {manager.api_key}" \\')
        print(f'  -H "Content-Type: application/json" \\')
        print("  -d '{")
        print('    "model": "chatgpt-4o-mini",')
        print('    "messages": [{"role": "user", "content": "Hello from AIMLAPI!"}]')
        print("  }'")
        
    else:
        print("\nâŒ KhÃ´ng thá»ƒ thÃªm models")

if __name__ == "__main__":
    main()
