#!/usr/bin/env python3
"""
LiteLLM Model Management Script
Script ƒë·ªÉ qu·∫£n l√Ω models trong LiteLLM: th√™m, x√≥a, c·∫≠p nh·∫≠t
"""

import requests
import json
from typing import Dict, Any, List

class LiteLLMAdvancedManager:
    def __init__(self, base_url: str = "https://call.shareapiai.com", api_key: str = "sk-hWv1u2fX3yG4zJ5kT6p7qR8sT9uV0wX1"):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    def list_models(self) -> List[Dict[str, Any]]:
        """L·∫•y danh s√°ch t·∫•t c·∫£ models"""
        try:
            response = requests.get(f"{self.base_url}/v1/models", headers=self.headers)
            response.raise_for_status()
            models = response.json().get("data", [])
            
            print(f"\nüìä DANH S√ÅCH MODELS HI·ªÜN T·∫†I ({len(models)} models):")
            print("="*60)
            for i, model in enumerate(models, 1):
                print(f"{i:2d}. {model.get('id', 'unknown')}")
            print("="*60)
            
            return models
        except Exception as e:
            print(f"‚ùå L·ªói khi l·∫•y models: {e}")
            return []
    
    def add_single_model(self, model_name: str, litellm_model: str, description: str = "", max_tokens: int = 4000) -> bool:
        """Th√™m m·ªôt model m·ªõi"""
        model_config = {
            "model_name": model_name,
            "litellm_params": {
                "model": litellm_model,
                "api_base": "https://api.aimlapi.com/v1",
                "api_key": "os.environ/AIMLAPI_KEY"
            },
            "model_info": {
                "description": description or f"{model_name} via AIMLAPI",
                "provider": "aimlapi",
                "max_tokens": max_tokens,
                "supports_function_calling": True
            }
        }
        
        try:
            response = requests.post(f"{self.base_url}/model/new", 
                                   headers=self.headers, 
                                   json=model_config)
            response.raise_for_status()
            print(f"‚úÖ ƒê√£ th√™m model: {model_name}")
            return True
        except Exception as e:
            print(f"‚ùå L·ªói khi th√™m model {model_name}: {e}")
            if hasattr(e, 'response') and e.response:
                print(f"   Response: {e.response.text}")
            return False
    
    def delete_model(self, model_name: str) -> bool:
        """X√≥a m·ªôt model"""
        try:
            # LiteLLM c√≥ th·ªÉ c√≥ endpoint delete model
            response = requests.delete(f"{self.base_url}/model/{model_name}", headers=self.headers)
            response.raise_for_status()
            print(f"‚úÖ ƒê√£ x√≥a model: {model_name}")
            return True
        except Exception as e:
            print(f"‚ùå L·ªói khi x√≥a model {model_name}: {e}")
            print("üí° Tip: C√≥ th·ªÉ c·∫ßn restart container ƒë·ªÉ x√≥a model")
            return False
    
    def update_model(self, model_name: str, new_config: Dict[str, Any]) -> bool:
        """C·∫≠p nh·∫≠t c·∫•u h√¨nh model"""
        # X√≥a model c≈© v√† th√™m l·∫°i v·ªõi config m·ªõi
        print(f"üîÑ C·∫≠p nh·∫≠t model {model_name}...")
        
        # Th√™m model m·ªõi v·ªõi config c·∫≠p nh·∫≠t
        try:
            response = requests.post(f"{self.base_url}/model/new", 
                                   headers=self.headers, 
                                   json=new_config)
            response.raise_for_status()
            print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t model: {model_name}")
            return True
        except Exception as e:
            print(f"‚ùå L·ªói khi c·∫≠p nh·∫≠t model {model_name}: {e}")
            return False
    
    def add_popular_models(self) -> bool:
        """Th√™m c√°c models ph·ªï bi·∫øn t·ª´ AIMLAPI"""
        popular_models = [
            {
                "name": "llama-3-1-70b",
                "litellm": "openai/meta-llama/llama-3.1-70b-instruct",
                "description": "Llama 3.1 70B - Meta's advanced model",
                "max_tokens": 128000
            },
            {
                "name": "llama-3-1-8b",
                "litellm": "openai/meta-llama/llama-3.1-8b-instruct",
                "description": "Llama 3.1 8B - Fast and efficient",
                "max_tokens": 128000
            },
            {
                "name": "mixtral-8x7b",
                "litellm": "openai/mistralai/mixtral-8x7b-instruct-v0.1",
                "description": "Mixtral 8x7B - Mixture of experts model",
                "max_tokens": 32000
            },
            {
                "name": "claude-3-haiku",
                "litellm": "openai/claude-3-haiku-20240307",
                "description": "Claude 3 Haiku - Fast and cost-effective",
                "max_tokens": 200000
            },
            {
                "name": "gemini-1-5-flash",
                "litellm": "openai/gemini-1.5-flash-latest",
                "description": "Gemini 1.5 Flash - Google's fast model",
                "max_tokens": 1048576
            }
        ]
        
        success_count = 0
        for model in popular_models:
            if self.add_single_model(
                model["name"], 
                model["litellm"], 
                model["description"], 
                model["max_tokens"]
            ):
                success_count += 1
        
        print(f"\nüìä ƒê√£ th√™m {success_count}/{len(popular_models)} models ph·ªï bi·∫øn")
        return success_count > 0
    
    def interactive_add_model(self):
        """Th√™m model theo c√°ch interactive"""
        print("\nüîß TH√äM MODEL M·ªöI")
        print("="*40)
        
        model_name = input("T√™n model (v√≠ d·ª•: my-custom-gpt): ").strip()
        if not model_name:
            print("‚ùå T√™n model kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
            return False
        
        print("\nC√°c providers c√≥ s·∫µn:")
        print("1. OpenAI models (gpt-4o, gpt-4-turbo, gpt-3.5-turbo)")
        print("2. Anthropic models (claude-3-5-sonnet, claude-3-opus)")
        print("3. Google models (gemini-1.5-pro, gemini-1.5-flash)")
        print("4. Meta models (llama-3.1-70b, llama-3.1-8b)")
        print("5. Custom/Other")
        
        provider = input("\nCh·ªçn provider (1-5): ").strip()
        
        if provider == "1":
            actual_model = input("Model OpenAI (v√≠ d·ª•: gpt-4o-mini): ").strip()
            litellm_model = f"openai/{actual_model}"
        elif provider == "2":
            actual_model = input("Model Anthropic (v√≠ d·ª•: claude-3-5-sonnet-20241022): ").strip()
            litellm_model = f"openai/{actual_model}"
        elif provider == "3":
            actual_model = input("Model Google (v√≠ d·ª•: gemini-1.5-pro-latest): ").strip()
            litellm_model = f"openai/{actual_model}"
        elif provider == "4":
            actual_model = input("Model Meta (v√≠ d·ª•: meta-llama/llama-3.1-70b-instruct): ").strip()
            litellm_model = f"openai/{actual_model}"
        else:
            litellm_model = input("LiteLLM model string (v√≠ d·ª•: openai/gpt-4o): ").strip()
        
        description = input("M√¥ t·∫£ model (t√πy ch·ªçn): ").strip()
        max_tokens = input("Max tokens (m·∫∑c ƒë·ªãnh 4000): ").strip()
        max_tokens = int(max_tokens) if max_tokens.isdigit() else 4000
        
        print(f"\nüìã TH√îNG TIN MODEL:")
        print(f"   T√™n: {model_name}")
        print(f"   LiteLLM: {litellm_model}")
        print(f"   M√¥ t·∫£: {description}")
        print(f"   Max tokens: {max_tokens}")
        
        confirm = input("\nX√°c nh·∫≠n th√™m model? (y/N): ").strip().lower()
        if confirm == 'y':
            return self.add_single_model(model_name, litellm_model, description, max_tokens)
        else:
            print("‚ùå H·ªßy th√™m model")
            return False
    
    def test_model(self, model_name: str) -> bool:
        """Test model c√≥ ho·∫°t ƒë·ªông kh√¥ng"""
        try:
            test_payload = {
                "model": model_name,
                "messages": [{"role": "user", "content": "Hello! Please respond with 'OK' if you're working."}],
                "max_tokens": 10
            }
            
            response = requests.post(f"{self.base_url}/v1/chat/completions",
                                   headers=self.headers,
                                   json=test_payload)
            response.raise_for_status()
            
            result = response.json()
            if "choices" in result and len(result["choices"]) > 0:
                print(f"‚úÖ Model {model_name} ho·∫°t ƒë·ªông: {result['choices'][0]['message']['content']}")
                return True
            else:
                print(f"‚ùå Model {model_name} kh√¥ng ph·∫£n h·ªìi ƒë√∫ng")
                return False
                
        except Exception as e:
            print(f"‚ùå Model {model_name} l·ªói: {e}")
            return False

def main():
    """Main interactive menu"""
    manager = LiteLLMAdvancedManager()
    
    while True:
        print("\n" + "="*60)
        print("üöÄ LITELLM MODEL MANAGER")
        print("="*60)
        print("1. Xem danh s√°ch models hi·ªán t·∫°i")
        print("2. Th√™m model m·ªõi (interactive)")
        print("3. Th√™m models ph·ªï bi·∫øn (batch)")
        print("4. Test m·ªôt model")
        print("5. X√≥a model (n·∫øu h·ªó tr·ª£)")
        print("0. Tho√°t")
        print("="*60)
        
        choice = input("Ch·ªçn t√πy ch·ªçn (0-5): ").strip()
        
        if choice == "1":
            manager.list_models()
            
        elif choice == "2":
            manager.interactive_add_model()
            
        elif choice == "3":
            print("\nüîß Th√™m models ph·ªï bi·∫øn...")
            manager.add_popular_models()
            
        elif choice == "4":
            models = manager.list_models()
            if models:
                model_name = input("\nNh·∫≠p t√™n model ƒë·ªÉ test: ").strip()
                if model_name:
                    manager.test_model(model_name)
                    
        elif choice == "5":
            models = manager.list_models()
            if models:
                model_name = input("\nNh·∫≠p t√™n model ƒë·ªÉ x√≥a: ").strip()
                if model_name:
                    confirm = input(f"X√°c nh·∫≠n x√≥a model '{model_name}'? (y/N): ").strip().lower()
                    if confirm == 'y':
                        manager.delete_model(model_name)
                        
        elif choice == "0":
            print("üëã T·∫°m bi·ªát!")
            break
            
        else:
            print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá")

if __name__ == "__main__":
    main()
